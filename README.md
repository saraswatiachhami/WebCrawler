# WebCrawler

## Description of the project(Simple Web Crawler for Vulnerability Scanning)
This project is a basic web crawler designed to scan websites for common security vulnerabilities. It is built to assist in web security assessments by automating the detection of issues such as missing security headers, outdated software versions, and insecure form attributes. The crawler starts from a given URL, explores all linked pages within the same domain, and generates a report summarizing the vulnerabilities found.

## Features
1. Crawling Functionality:
   -Recursively visits all pages linked from the starting URL and ensures it stays within the same domain to avoid crawling external sites.
2. Vulnerability Checks:
   -Missing Security Headers: Checks for the absence of critical HTTP headers like Strict-Transport-Security and X-Content-Type-Options.
   -Outdated Software Versions: Detects outdated server software (e.g., Apache) by parsing the Server header.
   Insecure Forms: Identifies forms that lack an action attribute or use the insecure GET method instead of POST.
3. Report Generation:
   -Compiles a detailed report listing all vulnerabilities found during the scan.
   -Provides actionable insights for improving website security.

## Steps to run this code
1. At first, Python should be installed in your computer. You can download it from: python.org
2. You should installed the required libraries: requests, beautifulsoup4, and re.
3. Create a folder for your project, e.g: Techincal Assessment and navigate to the folder.
4. Open a text editor or IDE (e.g., VS Code, PyCharm, or Notepad) and create a python file(e.g: web_scan_crawler.py) in that folder and write the Python code in web_scan_crawler.py .
5. Open a Terminal or Command Prompt in the same directory.
6. Run the code in the terminal by: python web_scan_crawler.py
7. After running thecode, it will prompt you to input a URL to scan. For example: Enter the URL to scan (e.g., https://example.com):
8. The script will then crawl the website and print out the vulnerability scan report, showing any missing headers, outdated software versions, or improperly configured forms it finds on the website.

## Assumptions or limitations in my implementation
## Assumptions
1. Security Headers:
-The absence of headers like Strict-Transport-Security and X-Content-Type-Options is considered a vulnerability.
-The server exposes its software version in the Server header.
2. Forms:
-Forms using the GET method are considered insecure for sensitive data submission.
-Forms without an action attribute are flagged as potentially problematic.
3. Security Headers:
-The absence of headers like Strict-Transport-Security and X-Content-Type-Options is considered a vulnerability.
-The server exposes its software version in the Server header.
4. Crawling Scope:
-The crawler is designed to stay within the same domain as the starting URL.
-It assumes that all linked pages are reachable from the starting page.
5. User Input:
-The user provides a valid and accessible URL to start the scan.

## Limitations
1. JavaScript Rendering:
-The crawler cannot process or analyze content dynamically generated by JavaScript (e.g., single-page applications or AJAX-loaded content).
2. No HTTPS Validation:
-The crawler does not validate SSL/TLS certificates or check for HTTPS misconfigurations.
3. No Parallel Processing:
-The tool processes pages sequentially, which can be slow for large websites.
4. No Depth Control:
-The crawler does not allow limiting the depth of crawling, which could lead to scanning an excessive number of pages.
5. Performance:
-The crawler is not optimized for large websites with thousands of pages. It may take a long time to scan such sites.
